{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['photosynthesis', 'occur', 'chloroplast', 'plant', 'cell']\n",
      "Keywords: ['Photosynthesis', 'the chloroplasts', 'plant cells']\n",
      "Generated Question: ___ occurs in the chloroplasts of plant cells.\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Load spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# def extract_keywords(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# def generate_question(sentence, keyword):\n",
    "#     question = sentence.replace(keyword, \"___\")\n",
    "#     return question\n",
    "\n",
    "# # Example text\n",
    "# text = \"Photosynthesis occurs in the chloroplasts of plant cells.\"\n",
    "\n",
    "# # Preprocess text\n",
    "# tokens = preprocess_text(text)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Extract keywords\n",
    "# keywords = extract_keywords(text)\n",
    "# print(\"Keywords:\", keywords)\n",
    "\n",
    "# # Generate question\n",
    "# if keywords:\n",
    "#     question = generate_question(text, keywords[0])\n",
    "#     print(\"Generated Question:\", question)\n",
    "\n",
    "# # Example output\n",
    "# # Tokens: ['photosynthesis', 'occur', 'chloroplast', 'plant', 'cell']\n",
    "# # Keywords: ['Photosynthesis', 'the chloroplasts', 'plant cells']\n",
    "# # Generated Question: ___ occurs in the chloroplasts of plant cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# today 29/5/2024\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load T5 model and tokenizer for question generation\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "t5_tokenizer.pad_token = t5_tokenizer.eos_token\n",
    "\n",
    "# Load GPT-2 model and tokenizer for distractor generation\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def generate_question(context, answer):\n",
    "    # Prepare the input text for T5\n",
    "    input_text = f\"generate question: {context} answer: {answer}\"\n",
    "    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt', padding=True)\n",
    "\n",
    "    # Generate question with attention mask\n",
    "    attention_mask = input_ids.ne(t5_tokenizer.pad_token_id).long()\n",
    "    outputs = t5_model.generate(input_ids, attention_mask=attention_mask, max_length=50, num_beams=5, early_stopping=True)\n",
    "    question = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return question\n",
    "\n",
    "def generate_distractors(answer, num_distractors=3):\n",
    "    distractors = set()  # Use a set to avoid duplicates\n",
    "    input_text = f\"Generate distractors for: {answer}. Distractor:\"\n",
    "\n",
    "    while len(distractors) < num_distractors:\n",
    "        input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt', padding=True)\n",
    "        attention_mask = input_ids.ne(gpt2_tokenizer.pad_token_id).long()\n",
    "        output = gpt2_model.generate(input_ids, attention_mask=attention_mask, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=gpt2_tokenizer.pad_token_id)\n",
    "        distractor = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        distractor = distractor.replace(input_text, '').strip()\n",
    "\n",
    "        if distractor and distractor.lower() != answer.lower() and distractor not in distractors:\n",
    "            distractors.add(distractor)\n",
    "    \n",
    "    return list(distractors)\n",
    "\n",
    "def generate_mcq(context):\n",
    "    sentences = sent_tokenize(context)\n",
    "    if not sentences:\n",
    "        return None\n",
    "\n",
    "    # Assume the answer is the last sentence for simplicity\n",
    "    answer = sentences[-1]\n",
    "    question = generate_question(context, answer)\n",
    "    distractors = generate_distractors(answer)\n",
    "\n",
    "    # Ensure we have 3 distractors\n",
    "    if len(distractors) < 3:\n",
    "        return None\n",
    "\n",
    "    # Combine correct answer with distractors and shuffle\n",
    "    options = distractors + [answer]\n",
    "    random.shuffle(options)\n",
    "\n",
    "    mcq = {\n",
    "        \"question\": question,\n",
    "        \"options\": options,\n",
    "        \"correct_answer\": answer\n",
    "    }\n",
    "\n",
    "    return mcq\n",
    "\n",
    "# Example usage\n",
    "context = \"The capital of France is Paris. It is known for its cafes and the Eiffel Tower.\"\n",
    "mcq = generate_mcq(context)\n",
    "\n",
    "if mcq:\n",
    "    print(\"Question:\", mcq[\"question\"])\n",
    "    print(\"Options:\", mcq[\"options\"])\n",
    "    print(\"Correct Answer:\", mcq[\"correct_answer\"])\n",
    "else:\n",
    "    print(\"MCQ generation failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mcq_generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: ____ mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.\n",
      "  A. data point\n",
      "  B. datum\n",
      "  C. Data\n",
      "  D. information\n",
      "Answer: Data\n",
      "\n",
      "Q2: \n",
      "____ learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.\n",
      "  A. simple machine\n",
      "  B. Machine\n",
      "  C. political machine\n",
      "  D. car\n",
      "Answer: Machine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sent) for sent in sentences]\n",
    "    pos_tags = [pos_tag(word) for word in words]\n",
    "    named_entities = [ne_chunk(tag) for tag in pos_tags]\n",
    "    named_entities_tags = [tree2conlltags(ne) for ne in named_entities]\n",
    "\n",
    "    keywords = []\n",
    "    for sent in named_entities_tags:\n",
    "        for word, tag, chunk in sent:\n",
    "            if chunk != 'O':\n",
    "                keywords.append(word)\n",
    "    \n",
    "    return list(set(keywords))\n",
    "\n",
    "def generate_distractors(answer, num_distractors=3):\n",
    "    distractors = set()\n",
    "    synsets = wn.synsets(answer)\n",
    "    if not synsets:\n",
    "        return list(distractors)\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name().lower() != answer.lower():\n",
    "                distractors.add(lemma.name().replace('_', ' '))\n",
    "            if len(distractors) >= num_distractors:\n",
    "                break\n",
    "        if len(distractors) >= num_distractors:\n",
    "            break\n",
    "    return list(distractors)\n",
    "\n",
    "def create_mcq(text):\n",
    "    keywords = extract_keywords(text)\n",
    "    questions = []\n",
    "    for keyword in keywords:\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentence = next((sent for sent in sentences if keyword in sent), None)\n",
    "        if sentence:\n",
    "            question = sentence.replace(keyword, \"____\")\n",
    "            distractors = generate_distractors(keyword)\n",
    "            if len(distractors) < 3:\n",
    "                continue\n",
    "            options = distractors + [keyword]\n",
    "            random.shuffle(options)\n",
    "            questions.append({\n",
    "                'question': question,\n",
    "                'options': options,\n",
    "                'answer': keyword\n",
    "            })\n",
    "    return questions\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]\n",
    "\n",
    "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
    "\n",
    "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[7][8]\n",
    "\n",
    "From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n",
    "\"\"\"\n",
    "\n",
    "# Generate MCQs\n",
    "mcqs = create_mcq(text)\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"Q{i+1}: {mcq['question']}\")\n",
    "    for j, option in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {option}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-5 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: ____ mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.\n",
      "  A. information\n",
      "  B. data point\n",
      "  C. datum\n",
      "  D. Data\n",
      "Answer: Data\n",
      "\n",
      "Q2: \n",
      "____ learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.\n",
      "  A. simple machine\n",
      "  B. car\n",
      "  C. Machine\n",
      "  D. political machine\n",
      "Answer: Machine\n",
      "\n",
      "Q3: As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (____).\n",
      "  A. three-toed sloth\n",
      "  B. artificial intelligence\n",
      "  C. AI\n",
      "  D. Army Intelligence\n",
      "Answer: AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [word_tokenize(sent) for sent in sentences]\n",
    "    pos_tags = [pos_tag(word) for word in words]\n",
    "    named_entities = [ne_chunk(tag) for tag in pos_tags]\n",
    "    named_entities_tags = [tree2conlltags(ne) for ne in named_entities]\n",
    "\n",
    "    keywords = []\n",
    "    for sent in named_entities_tags:\n",
    "        for word, tag, chunk in sent:\n",
    "            if chunk != 'O':\n",
    "                keywords.append(word)\n",
    "    \n",
    "    return list(set(keywords))\n",
    "\n",
    "def generate_distractors(answer, num_distractors=3):\n",
    "    distractors = set()\n",
    "    synsets = wn.synsets(answer)\n",
    "    if not synsets:\n",
    "        return list(distractors)\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name().lower() != answer.lower():\n",
    "                distractors.add(lemma.name().replace('_', ' '))\n",
    "            if len(distractors) >= num_distractors:\n",
    "                break\n",
    "        if len(distractors) >= num_distractors:\n",
    "            break\n",
    "    return list(distractors)\n",
    "\n",
    "def create_mcq(text, num_questions=4):\n",
    "    keywords = extract_keywords(text)\n",
    "    random.shuffle(keywords)\n",
    "    questions = []\n",
    "    for keyword in keywords:\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentence = next((sent for sent in sentences if keyword in sent), None)\n",
    "        if sentence:\n",
    "            question = sentence.replace(keyword, \"____\")\n",
    "            distractors = generate_distractors(keyword)\n",
    "            if len(distractors) < 3:\n",
    "                continue\n",
    "            options = distractors + [keyword]\n",
    "            random.shuffle(options)\n",
    "            questions.append({\n",
    "                'question': question,\n",
    "                'options': options,\n",
    "                'answer': keyword\n",
    "            })\n",
    "            if len(questions) >= num_questions:\n",
    "                break\n",
    "    return questions\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]\n",
    "\n",
    "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
    "\n",
    "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[7][8]\n",
    "\n",
    "From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n",
    "As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488 \n",
    "\n",
    "However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[24]: 25 \n",
    "\n",
    "Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[25]\n",
    "\"\"\"\n",
    "\n",
    "# Generate 4-5 MCQs\n",
    "mcqs = create_mcq(text, num_questions=5)\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"Q{i+1}: {mcq['question']}\")\n",
    "    for j, option in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {option}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd880954c27442e2bd72408e27899cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Codebook\\NLP\\vnlp\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\deepp\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['scientific', 'endeavor', 'machine', 'learning', 'grow', 'quest', 'artificial', 'intelligence', 'AI', 'early', 'day', 'AI', 'academic', 'discipline', 'researcher', 'interested', 'have', 'machine', 'learn', 'datum', 'attempt', 'approach', 'problem', 'symbolic', 'method', 'term', 'neural', 'network', 'perceptron', 'model', 'later', 'find', 'reinvention', 'generalized', 'linear', 'model', 'statistics.[23', 'probabilistic', 'reasoning', 'employ', 'especially', 'automate', 'medical', 'diagnosis.[24', '\\u200a', '488', '\\u200a  ', 'increase', 'emphasis', 'logical', 'knowledge', 'base', 'approach', 'cause', 'rift', 'AI', 'machine', 'learning', 'probabilistic', 'system', 'plague', 'theoretical', 'practical', 'problem', 'datum', 'acquisition', 'representation.[24', '\\u200a', '488', '\\u200a ', '1980', 'expert', 'system', 'come', 'dominate', 'AI', 'statistic', 'favor.[25', 'work', 'symbolic', 'knowledge', 'base', 'learning', 'continue', 'AI', 'lead', 'inductive', 'logic', 'programming(ilp', 'statistical', 'line', 'research', 'outside', 'field', 'AI', 'proper', 'pattern', 'recognition', 'information', 'retrieval.[24', '\\u200a', '708–710', '\\u200a', '755', '\\u200a ', 'Neural', 'network', 'research', 'abandon', 'AI', 'computer', 'science', 'time', 'line', 'continue', 'outside', 'AI', 'CS', 'field', 'connectionism', 'researcher', 'discipline', 'include', 'Hopfield', 'Rumelhart', 'Hinton', 'main', 'success', 'come', 'mid-1980', 'reinvention', 'backpropagation.[24', '\\u200a', '25', '\\u200a  ', 'Machine', 'learning', 'ML', 'reorganize', 'recognize', 'field', 'start', 'flourish', '1990', 'field', 'change', 'goal', 'achieve', 'artificial', 'intelligence', 'tackle', 'solvable', 'problem', 'practical', 'nature', 'shift', 'focus', 'away', 'symbolic', 'approach', 'inherit', 'AI', 'method', 'model', 'borrow', 'statistic', 'fuzzy', 'logic', 'probability', 'theory']\n",
      "Keywords: ['a scientific endeavor', 'machine learning', 'the quest', 'artificial intelligence', 'AI', 'the early days', 'AI', 'an academic discipline', 'some researchers', 'machines', 'data', 'They', 'the problem', 'various symbolic methods', 'what', '\"neural networks', 'these', 'perceptrons', 'other models', 'that', 'reinventions', 'the generalized linear models', 'statistics.[23', 'Probabilistic reasoning', 'automated medical diagnosis.[24', '488\\u200a  However, an increasing emphasis', 'the logical, knowledge-based approach', 'a rift', 'AI', 'machine learning', 'Probabilistic systems', 'theoretical and practical problems', 'data acquisition', 'representation.[24', 'expert systems', 'AI', 'statistics', 'Work', 'symbolic/knowledge-based learning', 'AI', 'inductive logic programming(ILP', 'the more statistical line', 'research', 'the field', 'AI', 'pattern recognition', 'information retrieval.[24', '755\\u200a Neural networks research', 'AI and computer science', 'the same time', 'This line', 'the AI/CS field', 'connectionism', 'researchers', 'other disciplines', 'Hopfield', 'Rumelhart', 'Hinton', 'Their main success', 'the mid-1980s', 'the reinvention', 'backpropagation.[24', '25\\u200a  Machine learning', 'ML', 'its own field', 'the 1990s', 'The field', 'its goal', 'artificial intelligence', 'solvable problems', 'a practical nature', 'It', 'the symbolic approaches', 'it', 'AI', 'methods', 'models', 'statistics', 'fuzzy logic', 'probability theory']\n",
      "Generated Question: as such, machine learning grew out of the quest for artificial intelligence ( ai ). in the early days of ai as an academic discipline, some researchers were interested in having machines learn from data. they attempted to approach the problem with various symbolic methods, as well as what were then termed \" neural networks \" ; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. [ 23 ] probabilistic reasoning was also employed, especially in automated medical diagnosis. [ 24 ] : 488 however, an increasing emphasis on the logical, knowledge - based approach caused a rift between ai and machine learning. probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. [ 24 ] : 488 by 1980, expert systems had come to dominate ai, and statistics was out of favor. [ 25 ] work on symbolic / knowledge - based learning did continue within ai, leading to inductive logic programming ( ilp ), but the more statistical line of research was now outside the field of ai proper, in pattern recognition and information retrieval. [ 24 ] : 708 – 710, 755 neural networks research had been abandoned by ai and computer science around the same time. this line, too, was continued outside the ai / cs field, as \" connectionism \", by researchers from other disciplines including hopfield, rumelhart, and hinton. their main success came in the mid - 1980s with the reinvention of backpropagation. [ 24 ] : 25 machine learning ( ml ), reorganized and recognized as its own field, started to flourish in the 1990s. the field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. it shifted focus away from the symbolic approaches it had inherited from ai, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. [\n",
      "Distractors: ['a scientific endeavor distractor 1', 'a scientific endeavor distractor 2', 'a scientific endeavor distractor 3']\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # Load spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Load pre-trained fill-mask model\n",
    "# mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# def extract_keywords(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# def generate_question(text, keyword):\n",
    "#     masked_text = text.replace(keyword, mask_filler.tokenizer.mask_token)\n",
    "#     results = mask_filler(masked_text)\n",
    "#     questions = [result['sequence'].replace(mask_filler.tokenizer.mask_token, keyword) for result in results]\n",
    "#     return questions[0] if questions else None\n",
    "\n",
    "# def generate_distractors(answer, n=3):\n",
    "#     # Placeholder for distractor generation using semantic similarity or rule-based methods\n",
    "#     return [answer + \" distractor \" + str(i+1) for i in range(n)]\n",
    "\n",
    "# # Example text\n",
    "# text = input(\"Please enter your paragraph\")\n",
    "# # Preprocess text\n",
    "# tokens = preprocess_text(text)\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# # Extract keywords\n",
    "# keywords = extract_keywords(text)\n",
    "# print(\"Keywords:\", keywords)\n",
    "\n",
    "# # Generate question\n",
    "# if keywords:\n",
    "#     question = generate_question(text, keywords[0])\n",
    "#     print(\"Generated Question:\", question)\n",
    "\n",
    "#     # Generate distractors\n",
    "#     correct_answer = keywords[0]\n",
    "#     distractors = generate_distractors(correct_answer)\n",
    "#     print(\"Distractors:\", distractors)\n",
    "\n",
    "# # Example output\n",
    "# # Tokens: ['photosynthesis', 'occur', 'chloroplast', 'plant', 'cell']\n",
    "# # Keywords: ['Photosynthesis', 'the chloroplasts', 'plant cells']\n",
    "# # Generated Question: [Generated question with mask token replaced by the keyword]\n",
    "# # Distractors: ['Photosynthesis distractor 1', 'Photosynthesis distractor 2', 'Photosynthesis distractor 3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['architecturally', 'school', 'catholic', 'character', '\\n', 'atop', 'Main', 'Building', 'gold', 'dome', 'golden', 'statue', 'Virgin', 'Mary', '\\n', 'immediately', 'Main', 'Building', 'face', '\\n', 'copper', 'statue', 'Christ', 'arm', 'upraise', 'legend', 'Venite', 'Ad', 'Omnes', '\\n', 'Main', 'Building', 'Basilica', 'Sacred', 'Heart', '\\n', 'immediately', 'basilica', 'Grotto', 'marian', 'place', 'prayer', 'reflection', '\\n', 'replica', 'grotto', 'Lourdes', 'France', 'Virgin', 'Mary', 'reputedly', 'appear', 'Saint', 'Bernadette', 'soubirous', '1858', '\\n', 'end', 'main', 'drive', 'direct', 'line', 'connect', '3', 'statue', 'Gold', 'Dome),is', 'simple', 'modern', 'stone', 'statue', 'Mary']\n",
      "Keywords: ['the school', 'a Catholic character', \"the Main Building's gold dome\", 'a golden statue', 'the Virgin Mary', 'front', 'the Main Building', 'it', 'a copper statue', 'Christ', 'arms', 'the legend', '\"Venite Ad Me Omnes', 'the Main Building', 'the Basilica', 'the Sacred Heart', 'the basilica', 'the Grotto', 'a Marian place', 'prayer', 'reflection', 'It', 'a replica', 'the grotto', 'Lourdes', 'France', 'the Virgin Mary', 'the end', 'the main drive', 'a direct line', 'that', '3 statues', 'a simple, modern stone statue', 'Mary']\n",
      "Generated Question: What is the name of the statue of Mary?\n",
      "Distractors: ['the school distractor 1', 'the school distractor 2']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load pre-trained question generation model\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-small-qg-prepend\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "def generate_question(text, keyword):\n",
    "    sentence = text.replace(keyword, \"[MASK]\")\n",
    "    input_text = f\"generate question: {sentence}\"\n",
    "    questions = question_generator(input_text)\n",
    "    return questions[0]['generated_text'] if questions else None\n",
    "\n",
    "def generate_distractors(answer, n=2):\n",
    "    # Placeholder for distractor generation using semantic similarity or rule-based methods\n",
    "    return [answer + \" distractor \" + str(i+1) for i in range(n)]\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\"Architecturally, the school has a Catholic character. \n",
    "Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\n",
    "Immediately in front of the Main Building and facing it,\n",
    "is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". \n",
    "Next to the Main Building is the Basilica of the Sacred Heart.\n",
    "Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection.\n",
    "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n",
    "At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome),is a simple, modern stone statue of Mary\"\"\"\n",
    "# Preprocess text\n",
    "tokens = preprocess_text(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Extract keywords\n",
    "keywords = extract_keywords(text)\n",
    "print(\"Keywords:\", keywords)\n",
    "\n",
    "# Generate question\n",
    "if keywords:\n",
    "    question = generate_question(text, keywords[0])\n",
    "    print(\"Generated Question:\", question)\n",
    "\n",
    "    # Generate distractors\n",
    "    correct_answer = keywords[0]\n",
    "    distractors = generate_distractors(correct_answer)\n",
    "    print(\"Distractors:\", distractors)\n",
    "\n",
    "# Example output\n",
    "# Tokens: ['photosynthesis', 'occur', 'chloroplast', 'plant', 'cell']\n",
    "# Keywords: ['Photosynthesis', 'the chloroplasts', 'plant cells']\n",
    "# Generated Question: What occurs in the chloroplasts of plant cells?\n",
    "# Distractors: ['Photosynthesis distractor 1', 'Photosynthesis distractor 2', 'Photosynthesis distractor 3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec29b5e8e0b4093a6c94c9f26ed7791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a67ab63defa47de8d9615ac87c534c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb6d49ec24341719d95ff9876e2527b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7314d3961b6408bae7fff744ba5a247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fd78d6d8da481792f4a7cea4d06b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "squad = load_dataset('squad')\n",
    "\n",
    "# Access the training data\n",
    "train_data = squad['train']\n",
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9babc4647bbb44878dd3cabd600da63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "83272389"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.to_csv(\"generator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load spaCy model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load T5 model for question generation\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "def generate_question(text, keyword):\n",
    "    # Create input text for T5\n",
    "    input_text = f\"generate question: {text}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate question\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "def generate_distractors(answer, n=3):\n",
    "    # Placeholder for distractor generation using semantic similarity or rule-based methods\n",
    "    return [answer + \" distractor \" + str(i+1) for i in range(n)]\n",
    "\n",
    "# Example text\n",
    "text = input (\"enter your paragraph\")\n",
    "\n",
    "# Preprocess text\n",
    "tokens = preprocess_text(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Extract keywords\n",
    "keywords = extract_keywords(text)\n",
    "print(\"Keywords:\", keywords)\n",
    "\n",
    "# Generate question\n",
    "if keywords:\n",
    "    question = generate_question(text, keywords[0])\n",
    "    print(\"Generated Question:\", question)\n",
    "\n",
    "    # Generate distractors\n",
    "    correct_answer = keywords[0]\n",
    "    distractors = generate_distractors(correct_answer)\n",
    "    print(\"Distractors:\", distractors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Generate a multiple choice question based on the following context:\n",
      "Choices: ['Correct Answer:', 'Context: As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]:\\u200a488\\u200a  However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]:\\u200a488\\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]:\\u200a708–710,\\u200a755\\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[24]:\\u200a25\\u200a  Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[', '', 'Generate 3 incorrect but plausible multiple choice answers for the following question based on the context:']\n",
      "Correct Answer: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import random\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt, max_new_tokens=100, num_return_sequences=1, temperature=0.7):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "def generate_question(context, num_choices=4):\n",
    "    \"\"\"\n",
    "    Generate a multiple choice question based on the given context.\n",
    "    \n",
    "    Args:\n",
    "    - context (str): The context or passage from which to generate the question.\n",
    "    - num_choices (int): The number of answer choices to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - question (str): The generated question.\n",
    "    - choices (list): A list of answer choices including the correct answer.\n",
    "    - correct_answer (str): The correct answer.\n",
    "    \"\"\"\n",
    "    prompt = f\"Generate a multiple choice question based on the following context:\\n\\n{context}\\n\\nQ: \"\n",
    "    generated = generate_text(prompt, max_new_tokens=150)[0]\n",
    "    \n",
    "    # Parse the response\n",
    "    lines = generated.split('\\n')\n",
    "    question = lines[0].strip()\n",
    "    correct_answer = lines[1].strip()\n",
    "    distractors = generate_distractors(context, correct_answer, num_choices - 1)\n",
    "    \n",
    "    choices = [correct_answer] + distractors\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    return question, choices, correct_answer\n",
    "\n",
    "def generate_distractors(context, correct_answer, num_distractors):\n",
    "    \"\"\"\n",
    "    Generate distractor answers based on the context and correct answer.\n",
    "    \n",
    "    Args:\n",
    "    - context (str): The context or passage from which to generate the distractors.\n",
    "    - correct_answer (str): The correct answer to base the distractors on.\n",
    "    - num_distractors (int): The number of distractors to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - distractors (list): A list of distractor answers.\n",
    "    \"\"\"\n",
    "    prompt = f\"Generate {num_distractors} incorrect but plausible multiple choice answers for the following question based on the context:\\n\\nContext: {context}\\n\\nCorrect Answer: {correct_answer}\\n\\nIncorrect Answers: \"\n",
    "    generated = generate_text(prompt, max_new_tokens=100)[0]\n",
    "    \n",
    "    distractors = [line.strip() for line in generated.split('\\n') if line.strip()]\n",
    "    return distractors[:num_distractors]\n",
    "\n",
    "# Example usage\n",
    "context = input(\"enter your paragraph\")\n",
    "question, choices, correct_answer = generate_question(context)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Choices:\", choices)\n",
    "print(\"Correct Answer:\", correct_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI)\n",
      "   1. As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI)\n",
      "   2. Not As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI)\n",
      "   3. Opposite of As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI)\n",
      "   4. )IA( ecnegilletni laicifitra rof tseuq eht fo tuo werg gninrael enihcam ,rovaedne cifitneics a sA\n",
      "Answer: As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI)\n",
      "\n",
      "Q2: In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data\n",
      "   1. In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data\n",
      "   2. Not In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data\n",
      "   3. Opposite of In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data\n",
      "   4. atad morf nrael senihcam gnivah ni detseretni erew srehcraeser emos ,enilpicsid cimedaca na sa IA fo syad ylrae eht nI\n",
      "Answer: In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data\n",
      "\n",
      "Q3: They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488   However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning\n",
      "   1. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488   However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning\n",
      "   2. Not They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488   However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning\n",
      "   3. Opposite of They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488   However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning\n",
      "   4. gninrael enihcam dna IA neewteb tfir a desuac hcaorppa desab-egdelwonk ,lacigol eht no sisahpme gnisaercni na ,revewoH   884 :]42[.sisongaid lacidem detamotua ni yllaicepse ,deyolpme osla saw gninosaer citsilibaborP ]32[.scitsitats fo sledom raenil dezilareneg eht fo snoitnevnier eb ot dnuof retal erew taht sledom rehto dna snortpecrep yltsom erew eseht ;\"skrowten laruen\" demret neht erew tahw sa llew sa ,sdohtem cilobmys suoirav htiw melborp eht hcaorppa ot detpmetta yehT\n",
      "Answer: They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[23] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[24]: 488   However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning\n",
      "\n",
      "Q4: Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time\n",
      "   1. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time\n",
      "   2. Not Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time\n",
      "   3. Opposite of Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time\n",
      "   4. emit emas eht dnuora ecneics retupmoc dna IA yb denodnaba neeb dah hcraeser skrowten larueN  557 ,017–807 :]42[.laveirter noitamrofni dna noitingocer nrettap ni ,reporp IA fo dleif eht edistuo won saw hcraeser fo enil lacitsitats erom eht tub ,)PLI(gnimmargorp cigol evitcudni ot gnidael ,IA nihtiw eunitnoc did gninrael desab-egdelwonk/cilobmys no kroW ]52[.rovaf fo tuo saw scitsitats dna ,IA etanimod ot emoc dah smetsys trepxe ,0891 yB  884 :]42[.noitatneserper dna noitisiuqca atad fo smelborp lacitcarp dna laciteroeht yb deugalp erew smetsys citsilibaborP\n",
      "Answer: Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[24]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[25] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[24]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time\n",
      "\n",
      "Q5: This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton\n",
      "   1. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton\n",
      "   2. Not This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton\n",
      "   3. Opposite of This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton\n",
      "   4. notniH dna ,trahlemuR ,dleifpoH gnidulcni senilpicsid rehto morf srehcraeser yb ,\"msinoitcennoc\" sa ,dleif SC/IA eht edistuo deunitnoc saw ,oot ,enil sihT\n",
      "Answer: This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart, and Hinton\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a paragraph\n",
    "def generate_paragraph(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Function to create MCQs from the paragraph\n",
    "def create_mcqs(paragraph, num_questions=2):\n",
    "    sentences = paragraph.split('. ')\n",
    "    questions = []\n",
    "\n",
    "    for sentence in sentences[:num_questions]:\n",
    "        # Simple way to generate a question (more sophisticated methods can be used)\n",
    "        question = sentence.replace(\" is\", \" is?\")\n",
    "        # Dummy options\n",
    "        options = [\n",
    "            sentence,\n",
    "            \"Not \" + sentence,\n",
    "            \"Opposite of \" + sentence,\n",
    "            sentence[::1]\n",
    "        ]\n",
    "        question_data = {\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": sentence\n",
    "        }\n",
    "        questions.append(question_data)\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "prompt = input(\"enter your paragraph\")\n",
    "paragraph = generate_paragraph(prompt)\n",
    "mcqs = create_mcqs(paragraph)\n",
    "\n",
    "# Display MCQs\n",
    "for idx, mcq in enumerate(mcqs, 1):\n",
    "    print(f\"Q{idx}: {mcq['question']}\")\n",
    "    for opt_idx, option in enumerate(mcq['options'], 1):\n",
    "        print(f\"   {opt_idx}. {option}\")\n",
    "    print(f\"Answer: {mcq['answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Machine learning (ML) is? a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]  ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is? known under the name predictive analytics\n",
      "   1. Generate a statement related to: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions\n",
      "   2. Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]  ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics\n",
      "   3. Generate a statement related to: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions\n",
      "   4. Generate a statement related to: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions\n",
      "Answer: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]  ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics\n",
      "\n",
      "Q2: Although not all machine learning is? statistically based, computational statistics is? an important source of the field's methods\n",
      "   1. Generate a statement related to: Although not all machine learning is statistically based, computational statistics is an important source of the field's methods\n",
      "   2. Generate a statement related to: Although not all machine learning is statistically based, computational statistics is an important source of the field's methods\n",
      "   3. Generate a statement related to: Although not all machine learning is statistically based, computational statistics is an important source of the field's methods\n",
      "   4. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods\n",
      "Answer: Although not all machine learning is statistically based, computational statistics is an important source of the field's methods\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import random  # Import the random module\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a paragraph\n",
    "def generate_paragraph(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Function to create MCQs from the paragraph\n",
    "def create_mcqs(paragraph, num_questions=2):\n",
    "    sentences = paragraph.split('. ')\n",
    "    questions = []\n",
    "\n",
    "    for sentence in sentences[:num_questions]:\n",
    "        question = sentence.replace(\" is\", \" is?\")\n",
    "\n",
    "        # Generate distractor options\n",
    "        correct_answer = sentence\n",
    "        options = [correct_answer]\n",
    "\n",
    "        # Generate plausible distractors\n",
    "        for _ in range(3):\n",
    "            inputs = tokenizer.encode(f\"Generate a statement related to: {sentence}\", return_tensors=\"pt\")\n",
    "            outputs = model.generate(inputs, max_new_tokens=20, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "            distractor = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\".\")[0]\n",
    "            if distractor and distractor != sentence:\n",
    "                options.append(distractor)\n",
    "        \n",
    "        # Ensure we have exactly 4 options\n",
    "        while len(options) < 4:\n",
    "            options.append(f\"Incorrect statement {_+1}\")\n",
    "\n",
    "        # Shuffle options to ensure the correct answer isn't always the first\n",
    "        random.shuffle(options)\n",
    "\n",
    "        question_data = {\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": correct_answer\n",
    "        }\n",
    "        questions.append(question_data)\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "prompt = input(\"Enter your paragraph: \")\n",
    "paragraph = generate_paragraph(prompt)\n",
    "mcqs = create_mcqs(paragraph)\n",
    "\n",
    "# Display MCQs\n",
    "for idx, mcq in enumerate(mcqs, 1):\n",
    "    print(f\"Q{idx}: {mcq['question']}\")\n",
    "    for opt_idx, option in enumerate(mcq['options'], 1):\n",
    "        print(f\"   {opt_idx}. {option}\")\n",
    "    print(f\"Answer: {mcq['answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
