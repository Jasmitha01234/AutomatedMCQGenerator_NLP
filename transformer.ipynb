{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import random\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "squad = load_dataset('squad')\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"context: \" + context + \" question: \" + question for context, question in zip(examples['context'], examples['question'])]\n",
    "    targets = [answer['text'][0] for answer in examples['answers']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./question-answer-model\")\n",
    "tokenizer.save_pretrained(\"./question-answer-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import random\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./question-answer-model\"  # Path to your saved model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_question(context, model, tokenizer, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate a question based on the given context using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "    - context (str): The context from which to generate the question.\n",
    "    - model: The fine-tuned T5 model.\n",
    "    - tokenizer: The T5 tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "    - question (str): The generated question.\n",
    "    - answer (str): The answer to the generated question.\n",
    "    \"\"\"\n",
    "    input_text = f\"context: {context} question:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the question and answer\n",
    "    question, answer = generated_text.split('question:')[1].strip().split('answer:')\n",
    "    return question.strip(), answer.strip()\n",
    "\n",
    "def generate_distractors(context, correct_answer, model, tokenizer, num_distractors=3, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate distractors based on the context and correct answer using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "    - context (str): The context from which to generate the distractors.\n",
    "    - correct_answer (str): The correct answer.\n",
    "    - model: The fine-tuned T5 model.\n",
    "    - tokenizer: The T5 tokenizer.\n",
    "    - num_distractors (int): Number of distractors to generate.\n",
    "    \n",
    "    Returns:\n",
    "    - distractors (list): A list of distractor answers.\n",
    "    \"\"\"\n",
    "    distractors = []\n",
    "    for _ in range(num_distractors):\n",
    "        input_text = f\"context: {context} correct_answer: {correct_answer} distractor:\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        distractor = generated_text.split('distractor:')[1].strip()\n",
    "        distractors.append(distractor)\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "def generate_mcq(context, model, tokenizer, num_choices=4):\n",
    "    \"\"\"\n",
    "    Generate a multiple choice question (MCQ) with distractors based on the given context.\n",
    "    \n",
    "    Args:\n",
    "    - context (str): The context from which to generate the MCQ.\n",
    "    - model: The fine-tuned T5 model.\n",
    "    - tokenizer: The T5 tokenizer.\n",
    "    - num_choices (int): The total number of choices (including the correct answer).\n",
    "    \n",
    "    Returns:\n",
    "    - question (str): The generated question.\n",
    "    - choices (list): A list of answer choices including the correct answer.\n",
    "    - correct_answer (str): The correct answer.\n",
    "    \"\"\"\n",
    "    # Generate the question and correct answer\n",
    "    question, correct_answer = generate_question(context, model, tokenizer)\n",
    "    \n",
    "    # Generate distractors\n",
    "    num_distractors = num_choices - 1\n",
    "    distractors = generate_distractors(context, correct_answer, model, tokenizer, num_distractors)\n",
    "    \n",
    "    # Combine the correct answer with distractors and shuffle\n",
    "    choices = distractors + [correct_answer]\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    return question, choices, correct_answer\n",
    "\n",
    "# Example usage\n",
    "context = \"\"\"The Battle of Hastings was fought on 14 October 1066 between the Norman-French army of William, the Duke of Normandy, and an English army under the Anglo-Saxon King Harold Godwinson. It took place at Senlac Hill, approximately 10 kilometers northwest of Hastings, close to the present-day town of Battle, East Sussex, and was a decisive Norman victory.\"\"\"\n",
    "\n",
    "question, choices, correct_answer = generate_mcq(context, model, tokenizer)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Choices:\", choices)\n",
    "print(\"Correct Answer:\", correct_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
